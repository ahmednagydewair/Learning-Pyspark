{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The coalesce method reduces the number of partitions in a DataFrame.\n",
    "df = spark.read.format(\"csv\").option(\"header\" ,\"true\").option(\"inferSchema\" , \"true\")\\\n",
    ".load(\"C:/Users/dewair/Spark-The-Definitive-Guide-master/data/retail-data/all/*.csv\").coalesce(5)\n",
    "df.cache()\n",
    "df.createOrReplaceTempView(\"dftable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count() # we use it here as action "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  541909|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT count(*) FROM dftable\").show() # in sql  count(1) will count the first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(StockCode)|\n",
      "+----------------+\n",
      "|          541909|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "df.select (count (df.StockCode)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT StockCode)|\n",
      "+-------------------------+\n",
      "|                     4070|\n",
      "+-------------------------+\n",
      "\n",
      "+-------------------------+\n",
      "|count(DISTINCT StockCode)|\n",
      "+-------------------------+\n",
      "|                     4070|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "df.select (countDistinct (df.StockCode)).show()\n",
    "spark.sql(\"select count (distinct (StockCode)) from dftable\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----------------------+\n",
      "|first(StockCode, false)|last(StockCode, false)|\n",
      "+-----------------------+----------------------+\n",
      "|                 85123A|                 22138|\n",
      "+-----------------------+----------------------+\n",
      "\n",
      "+-----------------------+----------------------+\n",
      "|first(StockCode, false)|last(StockCode, false)|\n",
      "+-----------------------+----------------------+\n",
      "|                 85123A|                 22138|\n",
      "+-----------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#select the first and last row in DF , will be based on the df , not on values within df\n",
    "from pyspark.sql.functions import first, last\n",
    "df.select (first(df.StockCode), last(df.StockCode)).show()\n",
    "spark.sql(\"select first(StockCode) ,  last(StockCode) from dftable\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|min(Quantity)|max(Quantity)|\n",
      "+-------------+-------------+\n",
      "|       -80995|        80995|\n",
      "+-------------+-------------+\n",
      "\n",
      "+-------------+-------------+\n",
      "|min(Quantity)|max(Quantity)|\n",
      "+-------------+-------------+\n",
      "|       -80995|        80995|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extract the min and max value \n",
    "from pyspark.sql.functions import min, max\n",
    "df.select (min(df.Quantity), max(df.Quantity)).show()\n",
    "spark.sql(\"select min(Quantity) ,  max(Quantity) from dftable\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sum(Quantity)|\n",
      "+-------------+\n",
      "|      5176450|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|sum(Quantity)|\n",
      "+-------------+\n",
      "|      5176450|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extract the sum\n",
    "from pyspark.sql.functions import sum\n",
    "df.select (sum(df.Quantity)).show()\n",
    "spark.sql(\"select sum(Quantity) from dftable\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|sum(DISTINCT Quantity)|\n",
      "+----------------------+\n",
      "|                 29310|\n",
      "+----------------------+\n",
      "\n",
      "+----------------------+\n",
      "|sum(DISTINCT Quantity)|\n",
      "+----------------------+\n",
      "|                 29310|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extract the sumdistinct\n",
    "from pyspark.sql.functions import sumDistinct\n",
    "df.select (sumDistinct(df.Quantity)).show()\n",
    "spark.sql(\"select sum(distinct (Quantity )) from dftable\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+----------------+----------------+\n",
      "|(sum_qut / count_qut)|         avg_qut|        mean_qut|\n",
      "+---------------------+----------------+----------------+\n",
      "|     9.55224954743324|9.55224954743324|9.55224954743324|\n",
      "+---------------------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#mean ,avg\n",
    "from pyspark.sql.functions import sum , count , avg , expr\n",
    "df.select (sum(df.Quantity).alias(\"sum_qut\") ,count(df.Quantity).alias(\"count_qut\"),\\\n",
    "          avg(df.Quantity).alias(\"avg_qut\") , expr (\"mean(Quantity)\").alias(\"mean_qut\"))\\\n",
    ".selectExpr(\"sum_qut/count_qut\" , \"avg_qut\" ,\"mean_qut\" ).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## group by "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------------+\n",
      "|InvoiceNo|quan|count(Quantity)|\n",
      "+---------+----+---------------+\n",
      "|   536596|   6|              6|\n",
      "|   536938|  14|             14|\n",
      "|   537252|   1|              1|\n",
      "|   537691|  20|             20|\n",
      "|   538041|   1|              1|\n",
      "|   538184|  26|             26|\n",
      "|   538517|  53|             53|\n",
      "|   538879|  19|             19|\n",
      "|   539275|   6|              6|\n",
      "|   539630|  12|             12|\n",
      "|   540499|  24|             24|\n",
      "|   540540|  22|             22|\n",
      "|  C540850|   1|              1|\n",
      "|   540976|  48|             48|\n",
      "|   541432|   4|              4|\n",
      "|   541518| 101|            101|\n",
      "|   541783|  35|             35|\n",
      "|   542026|   9|              9|\n",
      "|   542375|   6|              6|\n",
      "|  C542604|   8|              8|\n",
      "+---------+----+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count the occurences of each invoice \n",
    "from pyspark.sql.functions import count \n",
    "df.groupBy(df.InvoiceNo).agg(count(df.Quantity).alias(\"quan\")\\\n",
    "                         , expr(\"count(Quantity)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+\n",
      "|     avg(Quantity)|InvoiceNo|\n",
      "+------------------+---------+\n",
      "|               1.5|   536596|\n",
      "|33.142857142857146|   536938|\n",
      "|              31.0|   537252|\n",
      "|              8.15|   537691|\n",
      "|              30.0|   538041|\n",
      "+------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select avg(Quantity) ,  InvoiceNo from dftable group by InvoiceNo \").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "## window functions\n",
    "from pyspark.sql.functions import to_date \n",
    "dfwithdate = df.withColumn(\"date\", to_date(col(\"InvoiceDate\"), \"MM/d/yyyy H:mm\")) #d must be one didgit \n",
    "dfwithdate.createOrReplaceTempView(\"dfwithdate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+----------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|      date|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+----------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|2010-12-01|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|2010-12-01|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfwithdate.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first part to a window function is to create window specifications\n",
    "# partitionBy : describe how we will be breaking up our group\n",
    "# orderBY :  determine the ordering within a given partition \n",
    "# rowsBetween : atates which rows will be included based on current row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here i'm defining window specification\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc\n",
    "windowSpec = Window.partitionBy(\"CustomerId\", \"date\")\\\n",
    "  .orderBy(desc(\"Quantity\"))\\\n",
    "  .rowsBetween(Window.unboundedPreceding, Window.currentRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's time to create aggregate functions to run over window specification\n",
    "# i want to calculate the maximum purchase quatity over time\n",
    "from pyspark.sql.functions import max , col\n",
    "maxpurchase = max(col(\"Quantity\")).over(windowSpec)# this will return a column , we can use it with select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will create dense_rank :determine which date had the maximum purchase quantity\n",
    "from pyspark.sql.functions import rank, dense_rank\n",
    "purchasedense_rank = dense_rank().over(windowSpec)\n",
    "purchaserank = rank().over(windowSpec)\n",
    "# these two functions will return two columns , we can use them in select "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+--------+------------+-----------------+-------------------+\n",
      "|CustomerId|date|Quantity|quantityRank|quantityDenseRank|maxPurchaseQuantity|\n",
      "+----------+----+--------+------------+-----------------+-------------------+\n",
      "|     12346|null|   74215|           1|                1|              74215|\n",
      "|     12346|null|  -74215|           2|                2|              74215|\n",
      "|     12347|null|     240|           1|                1|                240|\n",
      "|     12347|null|      36|           2|                2|                240|\n",
      "|     12347|null|      36|           2|                2|                240|\n",
      "|     12347|null|      36|           2|                2|                240|\n",
      "|     12347|null|      24|           5|                3|                240|\n",
      "|     12347|null|      24|           5|                3|                240|\n",
      "|     12347|null|      24|           5|                3|                240|\n",
      "|     12347|null|      24|           5|                3|                240|\n",
      "|     12347|null|      24|           5|                3|                240|\n",
      "|     12347|null|      24|           5|                3|                240|\n",
      "|     12347|null|      24|           5|                3|                240|\n",
      "|     12347|null|      24|           5|                3|                240|\n",
      "|     12347|null|      24|           5|                3|                240|\n",
      "|     12347|null|      24|           5|                3|                240|\n",
      "|     12347|null|      24|           5|                3|                240|\n",
      "|     12347|null|      24|           5|                3|                240|\n",
      "|     12347|null|      24|           5|                3|                240|\n",
      "|     12347|null|      24|           5|                3|                240|\n",
      "+----------+----+--------+------------+-----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfwithdate.where(\"CustomerId IS NOT NULL\").orderBy(\"CustomerId\")\\\n",
    "  .select(\n",
    "    col(\"CustomerId\"),\n",
    "    col(\"date\"),\n",
    "    col(\"Quantity\"),\n",
    "    purchaserank.alias(\"quantityRank\"),\n",
    "    purchasedense_rank.alias(\"quantityDenseRank\"),\n",
    "    maxpurchase.alias(\"maxPurchaseQuantity\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnonull = dfwithdate.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will stop here in page 128 , in grouping set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
