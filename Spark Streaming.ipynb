{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are working with heterogeneity human activity recognition dataset\n",
    "#the folder containing the data is fairly large 1.18 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "static = spark.read.json(\"E:/big data courses/Spark-The-Definitive-Guide-master/data/activity-data/\")\n",
    "#read this data as simple dataframes \"patch processing applications\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataschema = static.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(Arrival_Time,LongType,true),StructField(Creation_Time,LongType,true),StructField(Device,StringType,true),StructField(Index,LongType,true),StructField(Model,StringType,true),StructField(User,StringType,true),StructField(gt,StringType,true),StructField(x,DoubleType,true),StructField(y,DoubleType,true),StructField(z,DoubleType,true)))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataschema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Arrival_Time=1424686735090, Creation_Time=1424686733090638193, Device='nexus4_1', Index=18, Model='nexus4', User='g', gt='stand', x=0.0003356934, y=-0.0005645752, z=-0.018814087)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "static.take(1)\n",
    "# the columns contain timestamp columns , model, user , device information , gt :specify activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#streaming should take schema explicitly \n",
    "streaming = spark.readStream.schema(dataschema).option(\"maxFilePerTrigger\",1)\\\n",
    ".json(\"E:/big data courses/Spark-The-Definitive-Guide-master/data/activity-data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "activitycounts = streaming.groupBy(\"gt\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\" , 5)\n",
    "#the default shullfe partitions = 200 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "activityquery  = activitycounts.writeStream.queryName(\"activity_counts\")\\\n",
    ".format(\"memory\").outputMode(\"complete\").start()\n",
    "#this will set activity_counts table in memory , we will query it later , \n",
    "#mode: complete to overwrite the results everytime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#activityquery.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pyspark.sql.streaming.StreamingQuery at 0x2a87c3d2588>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| gt|count|\n",
      "+---+-----+\n",
      "+---+-----+\n",
      "\n",
      "+---+-----+\n",
      "| gt|count|\n",
      "+---+-----+\n",
      "+---+-----+\n",
      "\n",
      "+---+-----+\n",
      "| gt|count|\n",
      "+---+-----+\n",
      "+---+-----+\n",
      "\n",
      "+----------+-------+\n",
      "|        gt|  count|\n",
      "+----------+-------+\n",
      "|       sit| 984714|\n",
      "|     stand| 910783|\n",
      "|stairsdown| 749059|\n",
      "|      walk|1060402|\n",
      "|  stairsup| 836598|\n",
      "|      null| 835725|\n",
      "|      bike| 863710|\n",
      "+----------+-------+\n",
      "\n",
      "+----------+-------+\n",
      "|        gt|  count|\n",
      "+----------+-------+\n",
      "|       sit| 984714|\n",
      "|     stand| 910783|\n",
      "|stairsdown| 749059|\n",
      "|      walk|1060402|\n",
      "|  stairsup| 836598|\n",
      "|      null| 835725|\n",
      "|      bike| 863710|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "for x in range(5):\n",
    "    spark.sql(\"select * from activity_counts\").show()\n",
    "    sleep(2)\n",
    "#this loop to domenstarte complete mode , and how it will overwrite everything    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selection and filtering \n",
    "from pyspark.sql.functions import expr\n",
    "simpletransfrom = streaming.withColumn(\"stairs\",expr(\"gt like '%stairs%'\"))\\\n",
    "                  .where(\"stairs\")\\\n",
    "                  .where(\"gt is not null\")\\\n",
    "                  .select(\"gt\",\"model\", \"Arrival_Time\",\"Creation_Time\")\\\n",
    "                  .writeStream.queryName(\"simple_Transform\")\\\n",
    "                  .format(\"memory\").outputMode(\"append\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-------------+-------------------+\n",
      "|      gt| model| Arrival_Time|      Creation_Time|\n",
      "+--------+------+-------------+-------------------+\n",
      "|stairsup|nexus4|1424687983631|1424689829685305118|\n",
      "|stairsup|nexus4|1424687983837|1424687981832546126|\n",
      "|stairsup|nexus4|1424687984222|1424687982225307357|\n",
      "|stairsup|nexus4|1424687984626|1424687982633321003|\n",
      "|stairsup|nexus4|1424687985029|1424687983036153035|\n",
      "+--------+------+-------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from simple_Transform\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "deviceModelStats = streaming.cube(\"gt\" , \"model\").avg()\\\n",
    "                   .drop(\"avg(Arrival_Time)\").drop(\"avg(Creation_Time)\").drop(\"avg(Index)\")\\\n",
    "                   .writeStream.queryName(\"device_counts2\").format(\"memory\")\\\n",
    "                   .outputMode(\"complete\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+--------------------+--------------------+--------------------+\n",
      "|   gt| model|              avg(x)|              avg(y)|              avg(z)|\n",
      "+-----+------+--------------------+--------------------+--------------------+\n",
      "|  sit|  null|-5.49433244039557...| 2.79144628170004E-4|-2.33994461689905...|\n",
      "|stand|  null|-3.11082189691711...|3.218461665975361...|2.141300040636498E-4|\n",
      "|  sit|nexus4|-5.49433244039557...| 2.79144628170004E-4|-2.33994461689905...|\n",
      "|stand|nexus4|-3.11082189691711...|3.218461665975361...|2.141300040636498E-4|\n",
      "| null|  null|-0.00847688860109...|-7.30455258739188...|0.003090601491419...|\n",
      "+-----+------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#you should wait untill the above query finished from processing \n",
    "# or using device_counts2.awaitTermination() , but only for production\n",
    "spark.sql(\"select * from device_counts2\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## joins in streaming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we will join static with streaming streams \n",
    "historicalagg = static.groupBy(\"gt\",\"model\").avg()\n",
    "devicemodelstats = streaming.drop(\"Arrival_Time\" ,\"Creation_Time\",\"Index\")\\\n",
    "                   .cube(\"gt\",\"model\").avg()\\\n",
    "                   .join(historicalagg , [\"gt\",\"model\"])\\\n",
    "                   .writeStream.queryName(\"device_Counts3\").format(\"memory\")\\\n",
    "                   .outputMode(\"complete\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+\n",
      "|        gt| model|              avg(x)|              avg(y)|              avg(z)|   avg(Arrival_Time)|  avg(Creation_Time)|        avg(Index)|              avg(x)|              avg(y)|              avg(z)|\n",
      "+----------+------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+\n",
      "|      bike|nexus4| 0.02268875955086685|-0.00877912156368...|-0.08251001663412344|1.424751134339985...|1.424752127369589...| 326459.6867328154| 0.02268875955086685|-0.00877912156368...|-0.08251001663412344|\n",
      "|      null|nexus4|-0.00847688860109...|-7.30455258739188...|0.003090601491419...|1.424749002876339...|1.424749919482127...| 219276.9663669269|-0.00847688860109...|-7.30455258739188...|0.003090601491419...|\n",
      "|stairsdown|nexus4|0.021613908669165436|-0.03249018824752617| 0.12035922691504071|1.424744591412857E12|1.424745503635636...|230452.44623187225|0.021613908669165436|-0.03249018824752617| 0.12035922691504071|\n",
      "|     stand|nexus4|-3.11082189691711...|3.218461665975361...|2.141300040636498E-4|1.424743637921209...|1.424744579547459...|31317.877585550017|-3.11082189691711...|3.218461665975361...|2.141300040636498E-4|\n",
      "|      walk|nexus4|-0.00390116006094...|0.001052508689953...|-6.95435553042997...|1.424746420641789...|1.424747351060674...|149760.09974990616|-0.00390116006094...|0.001052508689953...|-6.95435553042997...|\n",
      "+----------+------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from device_counts3\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## event time and statful processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will continue on the beow configuration\n",
    "#spark.conf.set(\"spark.sql.shuffle.partitions\" , 5)\n",
    "#static = spark.read.json(\"E:/big data courses/Spark-The-Definitive-Guide-master/data/activity-data/\")\n",
    "spark.conf.set(\"spark.sql.streaming.schemaInference\",\"true\")\n",
    "streaming = spark.readStream.option(\"maxFilePerTrigger\",10)\\\n",
    ".json(\"E:/big data courses/Spark-The-Definitive-Guide-master/data/activity-data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Arrival_Time: long (nullable = true)\n",
      " |-- Creation_Time: long (nullable = true)\n",
      " |-- Device: string (nullable = true)\n",
      " |-- Index: long (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- User: string (nullable = true)\n",
      " |-- gt: string (nullable = true)\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#inferring schema is disabled for streaming datasources , we can enable it with \n",
    "streaming.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in streaming processing we work with event time not processing time \n",
    "#convert time column to proper spark time stamp.\n",
    "witheventtime  = streaming.selectExpr(\"*\", \"cast (cast(Creation_Time as double)/1000000000 as timestamp)as event_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x2a87f4f6248>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import window , col\n",
    "witheventtime.groupBy(window (col(\"event_time\") , \"10 minutes\")).count()\\\n",
    ".writeStream\\\n",
    ".queryName(\"pyevent_per_window\")\\\n",
    ".format(\"memory\")\\\n",
    ".outputMode(\"complete\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from pyevent_per_window\").printSchema()\n",
    "# window in struct type (complex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|              window| count|\n",
      "+--------------------+------+\n",
      "|[2015-02-23 12:40...| 88681|\n",
      "|[2015-02-24 13:50...|150773|\n",
      "|[2015-02-24 15:00...|133323|\n",
      "|[2015-02-23 15:20...|106075|\n",
      "|[2015-02-22 02:40...|    35|\n",
      "+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from pyevent_per_window\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x2a87f4f6448>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#perform aggregations in multiple columns \n",
    "from pyspark.sql.functions import window , col\n",
    "witheventtime.groupBy(window (col(\"event_time\") , \"10 minutes\") , \"user\").count()\\\n",
    ".writeStream\\\n",
    ".queryName(\"pyevent_per_window_user\")\\\n",
    ".format(\"memory\")\\\n",
    ".outputMode(\"complete\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sliding windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x2a87f477708>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we will running windows every 10 minutes , starting every 5 minutes\n",
    "from pyspark.sql.functions import window , col\n",
    "witheventtime.groupBy(window (col(\"event_time\") , \"10 minutes\" , \"5 minutes\")).count()\\\n",
    ".writeStream\\\n",
    ".queryName(\"pyevent_per_window_user_5\")\\\n",
    ".format(\"memory\")\\\n",
    ".outputMode(\"complete\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"select * from pyevent_per_window_user_5\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## handling late data with watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x2a87fd0da08>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we will make latency 30 minutes , after this will ignore states within corresponding time frame\n",
    "from pyspark.sql.functions import window , col\n",
    "witheventtime.withWatermark(\"event_time\" , \"30 minutes\")\\\n",
    ".groupBy(window (col(\"event_time\") , \"10 minutes\" , \"5 minutes\")).count()\\\n",
    ".writeStream\\\n",
    ".queryName(\"pyevent_per_window_user_w\")\\\n",
    ".format(\"memory\")\\\n",
    ".outputMode(\"complete\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## drop duplicate if exist it is most important feature when working with record-at-atime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x2a87fa167c8>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make the record unique with user , and event_time \n",
    "from pyspark.sql.functions import expr\n",
    "witheventtime.withWatermark(\"event_time\" , \"30 minutes\")\\\n",
    ".dropDuplicates([\"user\",\"event_time\"])\\\n",
    ".groupBy(window (col(\"event_time\") , \"10 minutes\" , \"5 minutes\")).count()\\\n",
    ".writeStream\\\n",
    ".queryName(\"pyevent_per_window_user_d\")\\\n",
    ".format(\"memory\")\\\n",
    ".outputMode(\"complete\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
