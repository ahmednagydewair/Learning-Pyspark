{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD : Resilient Distributed Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we use rdds in fewer cases , and should not depend on these as much as we can , \n",
    "#becasue these provides a lot of work , and lacks a lot of optimizations .that are available in structured \n",
    "#we use rdds when we want to control over the pysical distributions of data acoree the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[5] at javaToPython at <unknown>:0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from dataframes and datasets ,by converting them to rdd \n",
    "spark.range(10).rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create dataframe from range functions\n",
    "spark.range(3).toDF(\"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#operate over these \n",
    "maprdd = spark.range(10).toDF(\"id\").rdd.map(lambda row:row[0])\n",
    "maprdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can create the DF from rdd with the same methodology with toDF() function\n",
    "spark.range(10).rdd.toDF()\n",
    "# create rdd of type Row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spark',\n",
       " 'the',\n",
       " 'defenitive',\n",
       " 'guide',\n",
       " ':big',\n",
       " 'data',\n",
       " 'processing',\n",
       " 'made',\n",
       " 'simple']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# also we can create RDD from Collection , we need tp parallelize method in sparkcontext\n",
    "#parallelize : turns single node collection  into parrallel collections\n",
    "mycollection = \"spark the defenitive guide :big data processing made simple\".split(\" \")\n",
    "words = spark.sparkContext.parallelize(mycollection,2)\n",
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words2 = sc.parallelize(mycollection,2)\n",
    "#bydefault spark session define sparkcontext  as sc , so these print the same results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mywords'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can name the rdd , to show up the spark UI\n",
    "words.setName(\"Mywords\")\n",
    "words.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#myfile  = sc.textFile(\"filename\") , this will create rdd \n",
    "#each record in rdd represent line in textfile \n",
    "#mycollection  = sc.wholeTextFiles(\"files\") here name of the file is the first object and the value of the text file is the second object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## manipulate RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distinct \n",
    "words.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spark', 'simple']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter : return boolen type to be used as filter function\n",
    "def startswiths (ind):\n",
    "    return ind.startswith(\"s\")\n",
    "\n",
    "words.filter(lambda word : startswiths(word)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map\n",
    "#spaecify function that retuen value and given correct input\n",
    "wordss = words.map(lambda word : (word ,word[0] , word.startswith(\"s\")) )\n",
    "# this returns the (word ,word[0] , word.startswith(\"s\") . word itself , 1st char , true or false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark', 's', True),\n",
       " ('the', 't', False),\n",
       " ('defenitive', 'd', False),\n",
       " ('guide', 'g', False),\n",
       " (':big', ':', False),\n",
       " ('data', 'd', False),\n",
       " ('processing', 'p', False),\n",
       " ('made', 'm', False),\n",
       " ('simple', 's', True)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordss.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark', 's', True), ('simple', 's', True)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#then we will filter wordss rdd to return only true\n",
    "wordss.filter(lambda word : word[2]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s', 'p', 'a', 'r', 'k']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#flatmap , each row should return multiple rows\n",
    "# it needs the output of the map function be an iterable that can be expanded\n",
    "words.flatMap(lambda word : list(word)).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['defenitive', 'processing']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sortby *-1 to sort desc\n",
    "words.sortBy(lambda word : len(word)*-1).take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce an rdd any kind of value to one value \n",
    "sc.parallelize(range(1,21)).reduce(lambda x,y :x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#longest word in our set of words \n",
    "def wordlengthreduce(leftword , rightword):\n",
    "    if len(leftword) > len(rightword):\n",
    "        return leftword\n",
    "    else:\n",
    "        return rightword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'processing'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.reduce(wordlengthreduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'spark': 1,\n",
       "             'the': 1,\n",
       "             'defenitive': 1,\n",
       "             'guide': 1,\n",
       "             ':big': 1,\n",
       "             'data': 1,\n",
       "             'processing': 1,\n",
       "             'made': 1,\n",
       "             'simple': 1})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#countByValue(): counts the number of values in a given rdd , itload the entire dataset into memory.\n",
    "# you should use it when the dataset is small \n",
    "words.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spark'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first : return the first itemin dataset\n",
    "words.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(range (1,20)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spark', 'the', 'defenitive', 'guide', ':big']\n",
      "[':big', 'data', 'defenitive', 'guide', 'made']\n",
      "['the', 'spark', 'simple', 'processing', 'made']\n"
     ]
    }
   ],
   "source": [
    "# take() and it's derivatives , this first scanning one partition and check the result to satisfy\n",
    "# take , takeOrdered , takeSample , top \n",
    "print (words.take(5))\n",
    "print (words.takeOrdered(5))\n",
    "print (words.top(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving files , means that writing to plain text ,\n",
    "#you must iterate over the partition in order to save the content to external database\n",
    "# if we have more than one partition , will endup with more than one partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.saveAsTextFile(\"Finalwords2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequencefiles : flat file consist of binary key-value pairs , it's used in map reduce jobs\n",
    "# can create sequence files using saveAsObjectFile\n",
    "#words.saveAsObjectFile(\"sequence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mywords ParallelCollectionRDD[28] at readRDDFromFile at PythonRDD.scala:247"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cache and persist , only handle data in memory, we can name it if we use setName function\n",
    "words.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(False, True, False, False, 1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can spaecify storage level (memory only , disk only , separately , offheap)\n",
    "words.getStorageLevel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not available in dataframes , similar to caching except it's not stored in memory, only disk.\n",
    "#helpful when performing iterative cmputation  , similar to caching\n",
    "sc.setCheckpointDir(\"checkpointing\")\n",
    "words.checkpoint()\n",
    "# when we reference this rdd , it will derive from checkpoint instead of source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
