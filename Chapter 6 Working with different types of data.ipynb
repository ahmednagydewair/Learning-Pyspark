{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = spark.read.format(\"csv\").option(\"header\" ,True).option(\"inferSchema\" , True)\\\n",
    "df = spark.read.format(\"csv\").option(\"header\" ,\"true\").option(\"inferSchema\" , \"true\")\\\n",
    ".load(\"2010-12-01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.printSchema()\n",
    "df.createOrReplaceTempView(\"dftable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first thing to learn => Lit function \n",
    "# lit used to convert from native types to spark corresponding ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[5: int, five: string, 5.0: double]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df.select (lit(5), lit(\"five\") , lit(5.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------------+\n",
      "|InvoiceNo|Description                  |\n",
      "+---------+-----------------------------+\n",
      "|536366   |HAND WARMER UNION JACK       |\n",
      "|536366   |HAND WARMER RED POLKA DOT    |\n",
      "|536367   |ASSORTED COLOUR BIRD ORNAMENT|\n",
      "|536367   |POPPY'S PLAYHOUSE BEDROOM    |\n",
      "|536367   |POPPY'S PLAYHOUSE KITCHEN    |\n",
      "+---------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df.where(\"InvoiceNo != 536365\").select (\"InvoiceNo\",\"Description\").show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------------------+\n",
      "|InvoiceNo|Description                        |\n",
      "+---------+-----------------------------------+\n",
      "|536365   |WHITE HANGING HEART T-LIGHT HOLDER |\n",
      "|536365   |WHITE METAL LANTERN                |\n",
      "|536365   |CREAM CUPID HEARTS COAT HANGER     |\n",
      "|536365   |KNITTED UNION FLAG HOT WATER BOTTLE|\n",
      "|536365   |RED WOOLLY HOTTIE WHITE HEART.     |\n",
      "+---------+-----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(\"InvoiceNo = 536365\").select (\"InvoiceNo\",\"Description\").show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#multi part of boolen filters\n",
    "from pyspark.sql.functions import instr\n",
    "pricefilter = col(\"UnitPrice\") >600\n",
    "descfilter = instr(df.Description , \"positive\") >=1\n",
    "df.where (df.StockCode.isin(\"DOT\")).where (pricefilter|descfilter).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|UnitPrice|IsExpensive|\n",
      "+---------+-----------+\n",
      "|   607.49|       true|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dotcodefilter = df.StockCode == \"DOT\"\n",
    "pricefilter = col(\"UnitPrice\") >600\n",
    "descfilter = instr(df.Description , \"positive\") >=1\n",
    "df.withColumn(\"IsExpensive\",dotcodefilter & (pricefilter |descfilter )).where(\"IsExpensive\")\\\n",
    ".select (\"UnitPrice\" ,\"IsExpensive\" ).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|UnitPrice|IsExpensive|\n",
      "+---------+-----------+\n",
      "|   569.77|       true|\n",
      "|   607.49|       true|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "df.withColumn(\"IsExpensive\", expr(\"not UnitPrice <= 250\" ) ).where(\"IsExpensive\")\\\n",
    ".select (\"UnitPrice\" ,\"IsExpensive\" ).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|UnitPrice|IsExpensive|\n",
      "+---------+-----------+\n",
      "|   569.77|       true|\n",
      "|   607.49|       true|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"IsExpensive\", col(\"UnitPrice\") >= 250 ).where(\"IsExpensive\")\\\n",
    ".select (\"UnitPrice\" ,\"IsExpensive\" ).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# if you are working with collumns with null\n",
    "df.where(df.Description.eqNullSafe(\"hello\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working With Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+------------------+\n",
      "|CustomerID|Quantity|UnitPrice|      RealQuantity|\n",
      "+----------+--------+---------+------------------+\n",
      "|   17850.0|       6|     2.55|239.08999999999997|\n",
      "|   17850.0|       6|     3.39|          418.7156|\n",
      "|   17850.0|       8|     2.75|             489.0|\n",
      "|   17850.0|       6|     3.39|          418.7156|\n",
      "|   17850.0|       6|     3.39|          418.7156|\n",
      "+----------+--------+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's assume that the quantity = (quantity * unitprice)^2 +5\n",
    "from pyspark.sql.functions import expr , pow\n",
    "fabricatedquantity = pow(df.Quantity*df.UnitPrice, 2)+5\n",
    "df.select(df.CustomerID ,df.Quantity,df.UnitPrice,fabricatedquantity.alias(\"RealQuantity\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerID|      RealQuantity|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# note that selectexpr uses all columns with \"\" not in [col , df.name] , all inside will be qouted \n",
    "df.selectExpr(\"CustomerID\" ,\"(power((Quantity*UnitPrice), 2)+5) as RealQuantity\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|round|2.5|\n",
      "+-----+---+\n",
      "|  2.5|2.5|\n",
      "|  2.5|2.5|\n",
      "+-----+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#round numbers .   round => round up .   bround => round down\n",
    "from pyspark .sql.functions import round, bround\n",
    "df.select (lit(\"2.5\").alias(\"round\") , lit(\"2.5\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n",
      "|round(2.5, 0)|bround(2.5, 0)|\n",
      "+-------------+--------------+\n",
      "|          3.0|           2.0|\n",
      "|          3.0|           2.0|\n",
      "+-------------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select (round (lit(\"2.5\")) , bround (lit(\"2.5\") )).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.04112314436835551\n",
      "+--------------------+\n",
      "|               corrr|\n",
      "+--------------------+\n",
      "|-0.04112314436835551|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate the correlation between two columns\n",
    "from pyspark.sql.functions import corr \n",
    "print ( df.stat.corr(\"Quantity\" , \"UnitPrice\") )\n",
    "df.select (corr (\"Quantity\" , \"UnitPrice\").alias(\"corrr\")).show(2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------------+------------------+\n",
      "|summary|         Description|         UnitPrice|        CustomerID|\n",
      "+-------+--------------------+------------------+------------------+\n",
      "|  count|                3098|              3108|              1968|\n",
      "|   mean|                null| 4.151946589446603|15661.388719512195|\n",
      "| stddev|                null|15.638659854603892|1854.4496996893627|\n",
      "|    min| 4 PURPLE FLOCK D...|               0.0|           12431.0|\n",
      "|    max|ZINC WILLIE WINKI...|            607.49|           18229.0|\n",
      "+-------+--------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to see summary statistics for column or set of columns , use descibe method\n",
    "df.describe(\"Description\",\"UnitPrice\",\"CustomerID\").show()\n",
    "#df.describe().show() # for all columns in DF if you want\n",
    "# if  you want exact measure from this ,pyspark.sql.functions import count , mean , min,max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "| StockCode_freqItems|  Quantity_freqItems|\n",
      "+--------------------+--------------------+\n",
      "|[90214E, 20728, 2...|[200, 128, 23, 32...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cross tab and frequent items pairs\n",
    "#df.stat.crosstab(\"StockCode\", \"Quantity\").show()\n",
    "df.stat.freqItems([\"StockCode\", \"Quantity\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|monotonically_increasing_id()|\n",
      "+-----------------------------+\n",
      "|                            0|\n",
      "|                            1|\n",
      "|                            2|\n",
      "+-----------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to add numbering column start from 0\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "df.select(monotonically_increasing_id()).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working With Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|initcap(Description)|\n",
      "+--------------------+\n",
      "|White Hanging Hea...|\n",
      "| White Metal Lantern|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the first function in inticap that captitalize every word in santance\n",
    "from pyspark.sql.functions import initcap\n",
    "df.select(initcap(df.Description)).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|         Description|  lower(Description)|  upper(Description)|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|WHITE HANGING HEA...|white hanging hea...|WHITE HANGING HEA...|\n",
      "| WHITE METAL LANTERN| white metal lantern| WHITE METAL LANTERN|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can also make the whole sentence as capital or small , with [lower, upper]\n",
    "from pyspark.sql.functions import lower , upper\n",
    "df.select (df.Description , lower(df.Description) , upper(df.Description)).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----+----+-----+\n",
      "|     ltrim|     rtrim|lpad|rpad| trim|\n",
      "+----------+----------+----+----+-----+\n",
      "|hello     |     hello| hel|  he|hello|\n",
      "|hello     |     hello| hel|  he|hello|\n",
      "|hello     |     hello| hel|  he|hello|\n",
      "+----------+----------+----+----+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# adding or removing spaces from string , using [ltrim, rtrim , lpad,rpad, trim]\n",
    "from pyspark.sql.functions import ltrim, rtrim , lpad,rpad, trim\n",
    "df.select(ltrim(lit(\"     hello     \")).alias(\"ltrim\"),\n",
    "          rtrim(lit(\"     hello     \")).alias(\"rtrim\"),\n",
    "          lpad(lit(\"hello\") , 3,\" \").alias(\"lpad\"),\n",
    "          rpad(lit(\"hello\"),2,\" \").alias(\"rpad\"),\n",
    "          trim(lit(\"   hello      \")).alias(\"trim\"),\n",
    "         ).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|         Description|          clean_desc|\n",
      "+--------------------+--------------------+\n",
      "|WHITE HANGING HEA...|COLOR HANGING HEA...|\n",
      "| WHITE METAL LANTERN| COLOR METAL LANTERN|\n",
      "+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# regular expressions \n",
    "from pyspark.sql.functions import regexp_replace\n",
    "regex_string = \"BLACK|WHITE|RED|GREEN|BLUE\" \n",
    "# regexp_replace take 3 arguments (the original column ,the strings should i found them , the replacing string  )\n",
    "df.select(df.Description,\\\n",
    "          regexp_replace(df.Description,regex_string,\"COLOR\").alias(\"clean_desc\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|         Description|          Clean Text|\n",
      "+--------------------+--------------------+\n",
      "|WHITE HANGING HEA...|WHI73 HANGING H3A...|\n",
      "| WHITE METAL LANTERN| WHI73 M37A1 1AN73RN|\n",
      "+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark also provide avery effictive function to replace chars,\n",
    "# fun [translate] replace chars with indexed chars in other string \n",
    "from pyspark.sql.functions import translate\n",
    "df.select(df.Description ,translate(df.Description, \"LEET\" , \"1337\").alias(\"Clean Text\")).show(2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|         Description|CleanText|\n",
      "+--------------------+---------+\n",
      "|WHITE HANGING HEA...|    WHITE|\n",
      "| WHITE METAL LANTERN|    WHITE|\n",
      "+--------------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "# regexp_extract => pulling out first mentioned color\n",
    "regex_string = \"(BLACK|WHITE|RED|GREEN|BLUE)\"\n",
    "#note that the regex_string must must be tuple ==> like r'(\\d+)'\n",
    "df.select (df.Description,\\\n",
    "           regexp_extract(df.Description,regex_string,1).alias(\"CleanText\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|         Description|hassimplercolor|\n",
      "+--------------------+---------------+\n",
      "|WHITE HANGING HEA...|           true|\n",
      "| WHITE METAL LANTERN|           true|\n",
      "|RED WOOLLY HOTTIE...|           true|\n",
      "+--------------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# if we want to check for existance in such string .contain function will do \n",
    "# corresponding to contains there is \"instr\" function in python .\n",
    "from pyspark.sql.functions import instr\n",
    "# instr : position (1 or greater)of the first occurrence of substr column in the given string \n",
    "# where(\"hassimplercolor\") returns true values only \n",
    "containblack = instr(df.Description,\"BLACK\") >=1 # this returns true or false\n",
    "containwhite = instr(df.Description,\"WHITE\") >=1\n",
    "df.withColumn(\"hassimplercolor\" , containblack|containwhite)\\\n",
    ".where(\"hassimplercolor\").select (df.Description , col(\"hassimplercolor\")).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# working with Date andTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark stores timestamps and dates as strings and convert them into datatypes at runtime,\n",
    "# this is much more when working with textfiles and csv \n",
    "#sparks timestamptype support only second level percision,if you want to work with millsecond or microsecond treat them as long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date , current_timestamp\n",
    "datedf = spark.range(10).withColumn(\"today\" , current_date()).withColumn(\"now\",current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+\n",
      "| id|     today|                 now|\n",
      "+---+----------+--------------------+\n",
      "|  0|2020-10-20|2020-10-20 23:11:...|\n",
      "+---+----------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datedf.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- today: date (nullable = false)\n",
      " |-- now: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datedf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|       add|date_sub(today, 5)|\n",
      "+----------+------------------+\n",
      "|2020-10-25|        2020-10-15|\n",
      "|2020-10-25|        2020-10-15|\n",
      "+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#subtract some days from date column\n",
    "from pyspark.sql.functions import date_add, date_sub\n",
    "datedf.select(date_add(datedf.today ,5).alias(\"add\") ,date_sub(datedf.today ,5) ).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|datediff(weekago, today)|\n",
      "+------------------------+\n",
      "|                      -7|\n",
      "+------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#difference between two dates \n",
    "from pyspark.sql.functions import datediff, months_between ,to_date , col ,lit\n",
    "datedf.withColumn(\"weekago\" ,date_sub(datedf.today,7))\\\n",
    ".select(datediff(col(\"weekago\") , col(\"today\") )).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|months_between(start, end, true)|\n",
      "+--------------------------------+\n",
      "|                     -4.67741935|\n",
      "|                     -4.67741935|\n",
      "+--------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#months betwen \n",
    "datedf.select(to_date(lit(\"2016-01-01\")).alias(\"start\"),to_date(lit(\"2016-05-022\")).alias(\"end\"))\\\n",
    ".select(months_between(col(\"start\"), col(\"end\") )).show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|      date|\n",
      "+---+----------+\n",
      "|  0|2017-01-01|\n",
      "|  1|2017-01-01|\n",
      "|  2|2017-01-01|\n",
      "|  3|2017-01-01|\n",
      "|  4|2017-01-01|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to_date function is used to transform from string to date type\n",
    "from pyspark.sql.functions import lit, to_date\n",
    "spark.range(5).withColumn(\"date\" , lit(\"2017-01-01\")).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|to_date(`date`)|\n",
      "+---------------+\n",
      "|     2017-01-01|\n",
      "|     2017-01-01|\n",
      "|     2017-01-01|\n",
      "|     2017-01-01|\n",
      "|     2017-01-01|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(5).withColumn(\"date\" , lit(\"2017-01-01\")).select(to_date(col(\"date\"))).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------------------+\n",
      "|to_date('2016-20-12')|to_date('2016-02-12')|\n",
      "+---------------------+---------------------+\n",
      "|                 null|           2016-02-12|\n",
      "+---------------------+---------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# very impportant note , \n",
    "# spark will not throw an error if spark can't parse  the date , it will return null,\n",
    "datedf.select(to_date(lit(\"2016-20-12\")), to_date(lit(\"2016-02-12\"))).show(1)\n",
    "# it will expect certain format and will treat others with null\n",
    "# spark follow java standard timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|     date1|     date2|\n",
      "+----------+----------+\n",
      "|2017-11-12|2017-12-20|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we use to_date , to_timestamp to solve the above issue\n",
    "from pyspark.sql.functions import to_date\n",
    "dateformat =  \"yyyy-dd-MM\"\n",
    "cleandatedf = spark.range(1).select(\n",
    "    to_date(lit(\"2017-12-11\") , dateformat).alias(\"date1\"),\n",
    "    to_date(lit(\"2017-20-12\"), dateformat).alias(\"date2\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# working with nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|coalesce(CustomerID, Description)|\n",
      "+---------------------------------+\n",
      "|                          17850.0|\n",
      "|                          17850.0|\n",
      "+---------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# frist functions is coalesce to return the first not null value , from set of columns\n",
    "from pyspark.sql.functions import coalesce\n",
    "df.select(coalesce(df.CustomerID ,df.Description)).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['InvoiceNo',\n",
       " 'StockCode',\n",
       " 'Description',\n",
       " 'Quantity',\n",
       " 'InvoiceDate',\n",
       " 'UnitPrice',\n",
       " 'CustomerID',\n",
       " 'Country']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#some functions to treat nulls\n",
    "#ifnull , returns the second value if the first is null \n",
    "#nullif ,  return null if the two values are equal, return the second if not \n",
    "# nvl , nvl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.na.drop()\n",
    "df.na.drop(\"any\") # drop rows if any of it's columns in null\n",
    "df.na.drop(\"all\") # drop rows if all columns in row is null\n",
    "df.na.drop(\"all\" , subset = [\"StockCode\" ,\"InvoiceNo\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill function to fill  na\n",
    "df.na.fill(\"all null values become this string\")# of type string \n",
    "#df.na.fill(5:Integer)\n",
    "fill_colls_values = {\"StockCode\":5 ,\"Description\":\"no value\"}\n",
    "df.na.fill(fill_colls_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.na.replace([\"\"],[\"unknown\"],\"Description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# working with complex types (structs , array, maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can think of struct as dataframe within dataframe \n",
    "from pyspark.sql.functions import struct , col\n",
    "complexdf = df.select (struct(\"Description\" , \"InvoiceNo\").alias(\"complex\"))\n",
    "complexdf.createOrReplaceTempView(\"complexdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.select (\"complexdf.*\")\n",
    "#df.select (col(\"complexdf\").getfield(\"Description\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           array_col|\n",
      "+--------------------+\n",
      "|[WHITE, HANGING, ...|\n",
      "|[WHITE, METAL, LA...|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#convert descriprion column to cpmlex type ,\"array\"\n",
    "from pyspark.sql.functions import split\n",
    "df.select (split(col(\"Description\"), \" \").alias(\"array_col\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|array_col[0]|\n",
      "+------------+\n",
      "|       WHITE|\n",
      "|       WHITE|\n",
      "+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(split(df.Description , \" \").alias(\"array_col\")).selectExpr(\"array_col[0]\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|array_size|\n",
      "+----------+\n",
      "|         5|\n",
      "|         3|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# query the array size \n",
    "from pyspark.sql.functions import size\n",
    "df.select(size (split(df.Description , \" \")).alias(\"array_size\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|contains|\n",
      "+--------+\n",
      "|    true|\n",
      "|    true|\n",
      "+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check if this array contains value \"array_contains\"\n",
    "from pyspark.sql.functions import array_contains\n",
    "df.select(array_contains(split(df.Description , \" \"),\"WHITE\").alias(\"contains\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to convert complex type into set of rows (one per value in our array). , function \"explode\"\n",
    "#explode takes column that consist of arrays , and create new rows , "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode\n",
    "ahmeddf = df.withColumn(\"splitted\" , split(df.Description, \" \")).withColumn(\"explodded\" ,explode(col(\"splitted\")))\\\n",
    ".select (\"Description\", \"StockCode\",\"explodded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+---------+\n",
      "|         Description|StockCode|explodded|\n",
      "+--------------------+---------+---------+\n",
      "|WHITE HANGING HEA...|   85123A|    WHITE|\n",
      "|WHITE HANGING HEA...|   85123A|  HANGING|\n",
      "+--------------------+---------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ahmeddf.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## working with maps to be in format key -value pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         complex_map|\n",
      "+--------------------+\n",
      "|[WHITE HANGING HE...|\n",
      "|[WHITE METAL LANT...|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import create_map\n",
    "df.select(create_map(df.Description , df.InvoiceNo).alias(\"complex_map\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.select(create_map(df.Description , df.InvoiceNo).alias(\"complex_map\"))\\\n",
    "#.selectExpr(\"complex_map['any text here to be matched']\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['InvoiceNo',\n",
       " 'StockCode',\n",
       " 'Description',\n",
       " 'Quantity',\n",
       " 'InvoiceDate',\n",
       " 'UnitPrice',\n",
       " 'CustomerID',\n",
       " 'Country']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# working with user defined function UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "udfexampledf = spark.range(5).toDF(\"num\") # create df with column named \"num\" and values from 0 to 4\n",
    "def power3 (double_value):\n",
    "    return double_value**3\n",
    "power3(3) # here i execute the function with static number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|num|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udfexampledf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
